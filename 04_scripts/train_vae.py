# scripts/train_vae.py
import argparse  # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import torch  # PyTorchã®ãƒ¡ã‚¤ãƒ³ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from torch.utils.data import DataLoader  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹
import torchvision  # ç”»åƒå‡¦ç†ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import torch.nn.functional as F  # æ´»æ€§åŒ–é–¢æ•°ã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys  # ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã‚’æ“ä½œã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import os  # OSæ“ä½œã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '01_src'))  # srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 

try:  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
    from data.dataset import CelebAImages  # CelebAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹
    from models.model_vae import VAE  # VAEãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹ï¼ˆæ­£ã—ã„ã‚¯ãƒ©ã‚¹åã«ä¿®æ­£ï¼‰
    from utils.logger import WandbLogger  # Weights & Biasesãƒ­ã‚¬ãƒ¼
    from utils.train_utils import get_device, save_checkpoint, load_checkpoint, get_scheduler  # å­¦ç¿’ç”¨ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
except ImportError:  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ãŸå ´åˆ
    # çµ¶å¯¾ãƒ‘ã‚¹ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
    from src.data.dataset import CelebAImages
    from src.models.model_vae import VAE
    from src.utils.logger import WandbLogger
    from src.utils.train_utils import get_device, save_checkpoint, load_checkpoint, get_scheduler

import wandb  # Wandbãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from tqdm import tqdm  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

def main(args):  # ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼šå­¦ç¿’ã®å…¨ä½“çš„ãªæµã‚Œã‚’åˆ¶å¾¡
    # 1. ãƒ‡ãƒã‚¤ã‚¹è¨­å®š âš™ï¸
    device = get_device()  # GPU/CPUã‚’è‡ªå‹•é¸æŠã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—
    print(f"Using device: {device}")  # ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’è¡¨ç¤º

    # 2. ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ– (wandb) ğŸ“
    logger = WandbLogger(  # Weights & Biasesãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
        project_name="vae-project",  # wandbã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
        entity="lee137ritsss-",  # ã‚ãªãŸã®wandbãƒ¦ãƒ¼ã‚¶ãƒ¼å
        config={
            **vars(args),
            'model_type': 'VAE',
            'dataset': 'CelebA',
            'z_dim': 100
        }
    )

    # 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®æº–å‚™ ğŸ“¦
    # CelebAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆtrainã¨downloadå¼•æ•°ã¯ä¸è¦ï¼‰
    try:  # transformã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è©¦è¡Œ
        from data.dataset import transform  # å‰å‡¦ç†ç”¨ã®transformã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    except ImportError:  # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ãŸå ´åˆ
        from src.data.dataset import transform  # çµ¶å¯¾ãƒ‘ã‚¹ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    dataset = CelebAImages(root=args.data_path, transform=transform)  # CelebAImagesã‚¯ãƒ©ã‚¹ã«rootã¨transformã‚’æ¸¡ã™
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)  # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ

    # å¯è¦–åŒ–ç”¨ã«å›ºå®šãƒãƒƒãƒã‚’æº–å‚™
    fixed_batch, _ = next(iter(dataloader))
    fixed_batch = fixed_batch.to(device)


    # 4. ãƒ¢ãƒ‡ãƒ«ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆæœŸåŒ– ğŸ§ 
    model = VAE(z_dim=100).to(device)  # VAEã‚¯ãƒ©ã‚¹ã¯z_dimï¼ˆæ½œåœ¨æ¬¡å…ƒï¼‰ã®ã¿ã‚’å—ã‘å–ã‚‹
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  # Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’åˆæœŸåŒ–

    # KLã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ã®å°å…¥ï¼ï¼ï¼ï¼ï¼ï¼ï¼(klé …ãŒå…¨ãåŠ¹ã‹ãªã„ãŸã‚)
    """
    æœ€åˆã¯beta=0ã«ã™ã‚‹ã“ã¨ã§ã€klãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’å­¦ç¿’ã•ã›ãªã„ã€‚
    ãã®å¾Œã€å¾ã€…ã«betaã‚’1ã«å¤‰åŒ–ã•ã›ã‚‹ã“ã¨ã§ã€klãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
    """
    kl_start_epoch = 10  # KLé …ã‚’åŠ¹ã‹ã›å§‹ã‚ã‚‹ã‚¨ãƒãƒƒã‚¯
    annealing_epochs = 20  # ä½•ã‚¨ãƒãƒƒã‚¯ã‹ã‘ã¦Î²ã‚’1ã«ã™ã‚‹ã‹

    # 5. å­¦ç¿’ãƒ«ãƒ¼ãƒ— 
    start_epoch = 0  # é–‹å§‹ã‚¨ãƒãƒƒã‚¯ã‚’å®šç¾©
    print("Training started...")  # å­¦ç¿’é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—å†…ã§ã€å„æå¤±ã‚’å€‹åˆ¥ã«è¿½è·¡
    for epoch in range(start_epoch, args.epochs):
        model.train()
        total_loss = 0
        total_recon_loss = 0  
        total_kl_loss = 0      

        if epoch < kl_start_epoch:
            beta = 0.0
        else:
            # ç·šå½¢ã«Î²ã‚’0ã‹ã‚‰1ã¸å¢—åŠ ã•ã›ã‚‹
            progress = (epoch - kl_start_epoch) / annealing_epochs
            beta = min(1.0, progress)  # ä¸Šé™ã‚’1.0ã«è¨­å®š

        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{args.epochs}', leave=False)
        
        for batch_idx, (data, _) in enumerate(pbar):
            data = data.to(device)
            
            reconstruction, mean, log_var = model(data)

            # å†æ§‹æˆæå¤± (BCE)
            reconstruction_loss = F.binary_cross_entropy(reconstruction, data, reduction='sum') / data.size(0)
            # KLæå¤±
            kl_loss = -0.5 * torch.sum(1 + log_var - mean**2 - torch.exp(log_var)) / data.size(0)
            
            # ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨ã—ãŸæœ€çµ‚çš„ãªæå¤±
            loss = reconstruction_loss + beta * kl_loss

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # å„æå¤±ã‚’ç´¯ç©
            total_loss += loss.item()
            total_recon_loss += reconstruction_loss.item()  
            total_kl_loss += kl_loss.item()                  
            
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Recon': f'{reconstruction_loss.item():.4f}',
                'KL': f'{kl_loss.item():.4f}'
            })
            
            # Wandbã«ãƒãƒƒãƒæ¯ã®æå¤±ã‚’ãƒ­ã‚°
            if batch_idx % args.log_interval == 0:
                global_step = epoch * len(dataloader) + batch_idx
                logger.log({
                    'train_loss': loss.item(),
                    'reconstruction_loss': reconstruction_loss.item(),
                    'kl_loss': kl_loss.item(),
                    'beta': beta
                }, step=global_step)
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®å¹³å‡æå¤±ã‚’ãƒ­ã‚°
        avg_loss = total_loss / len(dataloader)  # ç·æå¤±
        avg_recon_loss = total_recon_loss / len(dataloader)  # å†æ§‹æˆèª¤å·®
        avg_kl_loss = total_kl_loss / len(dataloader)        # KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
        
        #ã‚¨ãƒãƒƒã‚¯ã®æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã§ãƒ­ã‚°ã‚’è¨˜éŒ²
        epoch_step = (epoch + 1) * len(dataloader)
        print(f"====> Epoch: {epoch+1} Average loss: {avg_loss:.4f}")
        logger.log({
            'epoch_avg_loss': avg_loss,
            'epoch_avg_reconstruction_loss': avg_recon_loss,
            'epoch_avg_kl_loss': avg_kl_loss
        }, step=epoch_step)

        # 7. ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ ğŸ’¾
        if (epoch + 1) % args.save_interval == 0:  # æŒ‡å®šã•ã‚ŒãŸé–“éš”ã§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
            save_checkpoint(  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–¢æ•°ã‚’å‘¼ã³å‡ºã—
                model=model,  # å­¦ç¿’ä¸­ã®ãƒ¢ãƒ‡ãƒ«
                optimizer=optimizer,  # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®çŠ¶æ…‹
                epoch=epoch,  # ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯æ•°
                checkpoint_dir=args.checkpoint_dir,  # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                model_name=f"vae_epoch_{epoch}.pth"  # ãƒ•ã‚¡ã‚¤ãƒ«å
            )

        # ç”»åƒç”Ÿæˆã®å¯è¦–åŒ–
        if (epoch + 1) % args.save_interval == 0:  # 1ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«å®Ÿè¡Œ
            model.eval()
            with torch.no_grad():
                #1ã€€å†æ§‹æˆç”»åƒ
                reconstructed_images, _, _ = model(fixed_batch)

                #2ã€€ç”Ÿæˆç”»åƒ
                z_random = torch.randn(args.batch_size, 100).to(device)  # z_dim=100
                generated_images = model._decoder(z_random)

                #ã‚°ãƒªãƒƒãƒ‰ã®ä½œæˆ
                original_grid = torchvision.utils.make_grid(fixed_batch.cpu())
                reconstructed_grid = torchvision.utils.make_grid(reconstructed_images.cpu().clamp(0, 1))
                generated_grid = torchvision.utils.make_grid(generated_images.cpu().clamp(0, 1))
                
                # è¾æ›¸å½¢å¼ã§ä¸€åº¦ã«ãƒ­ã‚°
                logger.log({
                    "Originals": wandb.Image(original_grid),
                    "Reconstructions": wandb.Image(reconstructed_grid),
                    "Generated": wandb.Image(generated_grid)
                }, step=epoch_step)
            model.train()

    logger.finish()  # wandbã®ãƒ­ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†
    print("Training finished.")  # å­¦ç¿’å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸


if __name__ == '__main__':  # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒç›´æ¥å®Ÿè¡Œã•ã‚ŒãŸå ´åˆã®ã¿å®Ÿè¡Œ
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰å­¦ç¿’è¨­å®šã‚’å—ã‘å–ã‚‹ãŸã‚ã®ãƒ‘ãƒ¼ã‚µãƒ¼
    parser = argparse.ArgumentParser(description="Train VAE Model")  # å¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½œæˆ
    parser.add_argument('--data_path', type=str, default='./data', help='path to dataset')  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹
    parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')  # å­¦ç¿’ç‡
    parser.add_argument('--batch_size', type=int, default=64, help='batch size')  # ãƒãƒƒãƒã‚µã‚¤ã‚º
    parser.add_argument('--epochs', type=int, default=50, help='number of epochs')  # ã‚¨ãƒãƒƒã‚¯æ•°
    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='directory to save checkpoints')  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    parser.add_argument('--log_interval', type=int, default=100, help='how many batches to wait before logging status')  # ãƒ­ã‚°å‡ºåŠ›é–“éš”
    parser.add_argument('--save_interval', type=int, default=5, help='how many epochs to wait before saving model')  # ãƒ¢ãƒ‡ãƒ«ä¿å­˜é–“éš”
    
    args = parser.parse_args()  # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ
    main(args)  # ãƒ¡ã‚¤ãƒ³é–¢æ•°ã‚’å®Ÿè¡Œ