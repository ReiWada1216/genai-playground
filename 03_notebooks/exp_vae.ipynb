{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0849d3f",
   "metadata": {},
   "source": [
    "# 世界モデル道場(生成モデル編)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ff2d8",
   "metadata": {},
   "source": [
    "**VAE, VQ-VAE, Diffusionの理論と実装, 比較**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6860ce6",
   "metadata": {},
   "source": [
    "## 共通部分の定義（ライブラリ, データ, GPU）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7da9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1372e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----CUDA -> MPS(Mac) -> CPU の順で自動選択\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # Apple Silicon (M1/M2) 向け\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba5293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----画像の前処理設定-----\n",
    "#CelebAのデータセットは、[3, 218, 178]なので、128*128に変更\n",
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(178),         # まず正方形(178x178)にセンタークロップ\n",
    "    transforms.Resize(128),              # 128*128へ縮小\n",
    "    transforms.ToTensor(),              # [0,1]\n",
    "])\n",
    "\n",
    "#----カスタムDatasetの定義----\n",
    "class CelebAImages(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        # jpgが直下に並んでいる前提\n",
    "        self.paths = sorted(glob.glob(os.path.join(root, '*.jpg')))\n",
    "        assert len(self.paths) > 0, f'No jpg found in: {root}'\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, 0   # ラベルは使わないのでダミー\n",
    "\n",
    "#----データのパス----\n",
    "root_dir = '/Users/wadarei/genai-playground/00_data/img_align_celeba/img_align_celeba'\n",
    "dataset = CelebAImages(root_dir, transform)\n",
    "\n",
    "#----8:2 で分割----\n",
    "n = len(dataset); n_train = int(n*0.8)\n",
    "train_data, val_data = torch.utils.data.random_split(dataset, [n_train, n-n_train],\n",
    "                                                     generator=torch.Generator().manual_seed(42))\n",
    "#----DataLoader化----\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_data,   batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----データの確認----\n",
    "# train_loaderから1バッチ取得\n",
    "images, _ = next(iter(train_loader))\n",
    "\n",
    "\n",
    "# 画像を表示\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < images.shape[0]:\n",
    "        # 画像データを可視化用に整形\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy()\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Valid:\", len(val_data))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f20e40e",
   "metadata": {},
   "source": [
    "## VAE(Variational Auto Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021aabc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_log(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" torch.log(0)によるnanを防ぐ目的 \"\"\"\n",
    "    return torch.log(torch.clamp(x, min=1e-10))\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"VAEモデルの実装\"\"\"\n",
    "    def __init__(self, z_dim:int) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        z_dim : int\n",
    "          潜在空間の次元数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        #Encoder:  xを入力として、ガウス分布のパラメータmu, sigmaを出力\n",
    "        #入力画像：[B, 3, 128, 128]\n",
    "        self.conv_enc = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),  # -> [B, 64, 64, 64]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # -> [B, 128, 32, 32]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1), # -> [B, 256, 16, 16]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1), # -> [B, 512, 8, 8]\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \"\"\"\n",
    "        #厳密には定義しなくていいけど、その後のmu, sigmaのlinearで定義するために必要\n",
    "        \"\"\"\n",
    "        self.enc_flat = 512*8*8\n",
    "        self.enc_mu = nn.Linear(self.enc_flat, z_dim)\n",
    "        self.enc_var = nn.Linear(self.enc_flat, z_dim)\n",
    "\n",
    "        #Decoder: zを入力として、ガウス分布のパラメータmu, sigmaを出力\n",
    "        self.fc_dec = nn.Linear(z_dim, self.enc_flat)\n",
    "        self.conv_dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1), # -> [B, 256, 16, 16]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # -> [B, 128, 32, 32]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, 64,  4, 2, 1),  # -> [B, 64, 64, 64]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64,  3,   4, 2, 1),  # -> [B, 3, 128, 128]\n",
    "            nn.Sigmoid()  # 出力を [0,1] に収める\n",
    "        )\n",
    "\n",
    "    def _encoder(self, x: torch.Tensor):\n",
    "      \"\"\"\n",
    "      エンコーダは、xから平均と分散を出力するところ！！\n",
    "      -------------\n",
    "      Parameters\n",
    "      x: torch.Tensor(b, c, h, w)\n",
    "      \"\"\"\n",
    "      x = self.conv_enc(x)\n",
    "      x = torch.flatten(x, start_dim=1) #バッチだけ残すので、start_dim=1\n",
    "      mean = self.enc_mu(x) #平均\n",
    "      log_var = self.enc_var(x) #対数分散\n",
    "      return mean, log_var\n",
    "\n",
    "    def _sample_z(self, mean: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "      \"\"\"\n",
    "      VAEでは「潜在変数 𝑧」を確率的にサンプリングする.\n",
    "      しかし、確率的なサンプリングでは微分が不可能になる.\n",
    "      そのため、再パラメータ化トリックが必要！！\n",
    "      -------------------\n",
    "      Parameters\n",
    "      mean: torch.Tensor(b, z_dim)\n",
    "        エンコーダが出力するガウス分布の平均\n",
    "      log_var: torch.Tensor(b, z_dim)\n",
    "        エンコーダが出力するガウス分布の対数分散\n",
    "      \"\"\"\n",
    "      std = torch.exp(0.5 * log_var) #標準偏差\n",
    "      eps = torch.randn_like(std) #stdと同じ形の乱数を生成！(平均0, 分散1)\n",
    "      z = mean + std * eps\n",
    "      return z\n",
    "\n",
    "    def _decoder(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        VAEのデコーダ部分:zを受け取り、xを再構成する\n",
    "        ----------\n",
    "        Parameters\n",
    "        z: torch.Tensor(b, z_dim)\n",
    "          潜在変数\n",
    "        ----------\n",
    "        returns\n",
    "        x : torch.Tensor(b, c, h, w)\n",
    "        \"\"\"\n",
    "        x = self.fc_dec(z)\n",
    "        # x.viewはreshapeと同じ働き\n",
    "        x = x.view(-1, 512, 8, 8)  #-1は、PyTorchに「この次元は自分で計算して」と伝える特別な値。\n",
    "        x = self.conv_dec(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        順伝播\n",
    "        ---------\n",
    "        Parameters\n",
    "        x : torch.Tensor ( b, c, h, w )\n",
    "              入力画像．\n",
    "        ---------\n",
    "        Returns\n",
    "        x : torch.Tensor ( b, c, h, w )\n",
    "              再構成画像．\n",
    "        z : torch.Tensor ( b, z_dim )\n",
    "              潜在変数．\n",
    "        \"\"\"\n",
    "        mean, log_var = self._encoder(x)\n",
    "        z = self._sample_z(mean, log_var)\n",
    "        x = self._decoder(z)\n",
    "        return x, z\n",
    "\n",
    "    def loss(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        目的関数の計算\n",
    "        ----------\n",
    "        parameters\n",
    "        x: torch.Tensor(b, c, h, w)\n",
    "          入力画像\n",
    "        ----------\n",
    "        returns\n",
    "        reconstruction: torch.Tensor(b, c, h, w)\n",
    "          再構成画像\n",
    "        KL: Torch.Tensor(, )\n",
    "          正則化, エンコーダ（ガウス分布）と事前分布（標準ガウス分布）のKLダイバージェンス．\n",
    "        \"\"\"\n",
    "        mean, log_var = self._encoder(x)\n",
    "        z = self._sample_z(mean, log_var)\n",
    "        x_hat = self._decoder(z)\n",
    "\n",
    "        #-----KLのダイバージェンス, mean, std: (B, z_dim)\n",
    "        #torch.meanはbatch_sizeに関するもの\n",
    "        KL = -0.5 * torch.mean(torch.sum(1 + log_var - mean**2 - torch.exp(log_var), dim=1))\n",
    "\n",
    "        #----再構成誤差(MSE): カラー画像の場合は、バイナリクロスエントロピーではなく平均二乗誤差\n",
    "        reconstruction = F.mse_loss(x_hat, x, reduction='mean') #ピクセルごとの誤差を平均している #負の対数尤度ではない(平均二乗誤差だから)\n",
    "\n",
    "        #----目的関数\n",
    "        loss = reconstruction + KL\n",
    "\n",
    "        return loss, reconstruction, KL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194626b",
   "metadata": {},
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcba5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- モデルとオプティマイザ ----\n",
    "z_dim = 100\n",
    "model = VAE(z_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # ← parameters() のスペル修正\n",
    "\n",
    "# ---- Trainer ----\n",
    "n_epochs = 5\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch [{epoch}/{n_epochs}]\", ncols=100)\n",
    "    total_loss, total_recon, total_kl = 0.0, 0.0, 0.0\n",
    "\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # ---- Forward ----\n",
    "        loss, recon, kl = model.loss(x)\n",
    "\n",
    "        # ---- Backprop ----\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()           # backward\n",
    "        optimizer.step()           #optimizer\n",
    "\n",
    "        # ---- ロス集計 ----\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kl += kl.item()\n",
    "\n",
    "        # ---- 進捗バー更新 ----\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.3f}\",\n",
    "            \"Recon\": f\"{recon.item():.3f}\",\n",
    "            \"KL\": f\"{kl.item():.3f}\"\n",
    "        })\n",
    "\n",
    "    # ---- エポックごとの平均 ----\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_recon = total_recon / len(train_loader)\n",
    "    avg_kl = total_kl / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}: Loss={avg_loss:.4f}, Recon={avg_recon:.4f}, KL={avg_kl:.4f}\")\n",
    "\n",
    "    # ---- 各エポック後の再構成画像を可視化 ----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, _ = next(iter(train_loader))\n",
    "        x = x.to(device)\n",
    "        x_hat, _ = model(x)\n",
    "\n",
    "        def denorm(t): return (t.clamp(0, 1) + 1) / 2  \n",
    "        comparison = torch.cat([denorm(x[:8]), denorm(x_hat[:8])])\n",
    "        grid = torchvision.utils.make_grid(comparison, nrow=8)\n",
    "        plt.figure(figsize=(12, 3))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.title(f\"Epoch {epoch} Reconstruction\")\n",
    "        plt.show()\n",
    "\n",
    "    model.train()  # 次エポックに戻す\n",
    "\n",
    "# ---- 学習後に保存 ----\n",
    "torch.save(model.state_dict(), \"vae_celeba.pth\")\n",
    "print(\"✅ モデル保存完了: vae_celeba.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-playground)",
   "language": "python",
   "name": "genai-playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
