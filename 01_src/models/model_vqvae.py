"""
VQ-VAEãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

class VectorQuantizer(nn.Module):
    """ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼"""
    def __init__(self, num_emb: int, emb_dim: int, beta: float):
        """
        Parameters
        ----------
        num_emb  : int
            ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®ã‚µã‚¤ã‚º:VAEã§è¨€ã†ã¨ã“ã‚ã®z_dim
        emb_dim : int
            ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®æ¬¡å…ƒæ•°
        beta : float
            ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒ³ãƒˆæå¤±ã®é‡ã¿
        """
        super().__init__()
        self.num_emb = num_emb
        self.emb_dim = emb_dim
        self.beta = beta

        #ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®å®šç¾©
        self.embedding = nn.Embedding(self.num_emb, self.emb_dim)
        #é‡ã¿ã®åˆæœŸåŒ–: -1/num_embã‹ã‚‰1/num_embã®é–“ã§ä¸€æ§˜åˆ†å¸ƒã«å¾“ã†ä¹±æ•°ã‚’ç”Ÿæˆ
        self.embedding.weight.data.uniform_(-1/self.num_emb, 1/self.num_emb) 

    def forward(self, z_e: torch.Tensor):
        """
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ›z_eã‚’å—ã‘å–ã‚Šã€é‡å­åŒ–ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°z_qã‚’å‡ºåŠ›
        Parameters
        ----------
        z_e : torch.Tensor
             ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› (B, emb_dim, h, w)
        """
        #[B, emb_dim, h, w] -> [B, h, w, emb_dim]
        z_e = z_e.permute(0, 2, 3, 1) 
        z_e_shape = z_e.shape
        #[B, h, w, emb_dim] -> [B*h*w, emb_dim]
        z_e_flat = z_e.reshape(-1, z_e_shape[-1]) 

        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‹ã‚‰æœ€ã‚‚è¿‘ã„ã‚³ãƒ¼ãƒ‰ã‚’é¸æŠ(L2ãƒãƒ«ãƒ )
        #(a-b)^2 = a^2 + b^2 - 2ab
        distances = (torch.sum(z_e_flat**2, dim=1, keepdim=True)
                    + torch.sum(self.embedding.weight**2, dim=1)
                    -2 * torch.matmul(z_e_flat, self.embedding.weight.T) 
        ) # -> [B*h*w, num_emb]

        #æœ€ã‚‚è·é›¢ã®è¿‘ã„ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é¸æŠ
        #ã“ã“ã§å‹¾é…ãŒæµã‚Œãªã„ğŸ˜­
        encoding_indices = torch.argmin(distances, dim=1) # -> [B*h*w]
        #one-hot encoding:æœ€ã‚‚è¿‘ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒ1ã€ãã‚Œä»¥å¤–ã¯0
        encodings = F.one_hot(encoding_indices, num_classes=self.num_emb).to(torch.float32) # -> [B*h*w, num_emb]

        #onehotãƒ™ã‚¯ãƒˆãƒ«ã¨ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®é‡ã¿ã‚’æ›ã‘åˆã‚ã›ã¦ã€é‡å­åŒ–ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã‚’å–å¾—
        z_q = torch.matmul(encodings, self.embedding.weight)
        z_q = z_q.reshape(z_e_shape) # -> [B, h, w, emb_dim]

        #.detachã¯ã€é€†ä¼æ’­ã®éš›ã®å‹¾é…ã‚’åˆ‡æ–­ã™ã‚‹ãŸã‚ã®é–¢æ•°
        #lossã®ç¬¬äºŒé …: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®å‹¾é…ã®ã¿ã‚’æ›´æ–°
        L_e = F.mse_loss(z_q.detach(), z_e)

        #lossã®ç¬¬ä¸‰é …: ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’æ›´æ–°
        L_q = F.mse_loss(z_q, z_e.detach())

        #total loss
        vq_loss = L_e + self.beta * L_q

        #straight-through estimator: z_qã‹ã‚‰z_eã«å‹¾é…ã‚’ä¼ãˆã‚‹ä»•çµ„ã¿
        z_q = z_e + (z_q - z_e).detach()
        z_q = z_q.permute(0, 3, 1, 2) # -> [B, emb_dim, h, w]

        return z_q, vq_loss, encoding_indices

class VQVAE(nn.Module):
    """VQ-VAEãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…"""
    def __init__(self, num_emb: int, emb_dim: int = 512, beta: float = 0.25):
        """
        Parameters
        ----------
        num_emb : int
            ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®ã‚µã‚¤ã‚º:VAEã§è¨€ã†ã¨ã“ã‚ã®z_dim
        emb_dim : int
            ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã®æ¬¡å…ƒæ•°
        beta : float
            ã‚³ãƒŸãƒƒãƒˆãƒ¡ãƒ³ãƒˆæå¤±ã®é‡ã¿
        """
        super().__init__()
        
        # Encoder: xã‚’å…¥åŠ›ã¨ã—ã¦ã€é€£ç¶šçš„ãªæ½œåœ¨è¡¨ç¾ã‚’å‡ºåŠ›
        # å…¥åŠ›ç”»åƒï¼š[B, 3, 64, 64]
        self.conv_enc = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),  # -> [B, 64, 32, 32]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), # -> [B, 128, 16, 16]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1), # -> [B, 256, 8, 8]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, emb_dim, 4, 2, 1), # -> [B, emb_dim, 4, 4]
            nn.BatchNorm2d(emb_dim),
            nn.ReLU(),
        )
        
        # ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼
        self.vq_layer = VectorQuantizer(num_emb, emb_dim, beta)
        
        # Decoder: é‡å­åŒ–ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã‹ã‚‰ç”»åƒã‚’å†æ§‹æˆ
        self.conv_dec = nn.Sequential(
            nn.ConvTranspose2d(emb_dim, 256, 4, 2, 1), # -> [B, 256, 8, 8]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # -> [B, 128, 16, 16]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64,  4, 2, 1),  # -> [B, 64, 32, 32]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.ConvTranspose2d(64,  3,   4, 2, 1),  # -> [B, 3, 64, 64]
            nn.Sigmoid()  # å‡ºåŠ›ã‚’ [0,1] ã«åã‚ã‚‹
        )
    
    def encode(self, x: torch.Tensor):
        """
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€: å…¥åŠ›ç”»åƒã‚’é€£ç¶šçš„ãªæ½œåœ¨è¡¨ç¾ã«å¤‰æ›
        
        Parameters
        ----------
        x : torch.Tensor
            å…¥åŠ›ç”»åƒ (batch_size, 3, 64, 64)
        
        Returns
        -------
        z_e : torch.Tensor
            ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› (B, emb_dim, h, w)
        """
        z_e = self.conv_enc(x)
        return z_e
    
    def quantize(self, z_e: torch.Tensor):
        """
        ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–
        
        Parameters
        ----------
        z_e : torch.Tensor
            ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å‡ºåŠ› (B, emb_dim, h, w)
        
        Returns
        -------
        z_q : torch.Tensor
            é‡å­åŒ–ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•° (B, emb_dim, h, w)
        vq_loss : torch.Tensor
            ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–æå¤±
        encoding_indices : torch.Tensor
            é¸æŠã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (B*h*w)
        """
        z_q, vq_loss, encoding_indices = self.vq_layer(z_e)
        return z_q, vq_loss, encoding_indices
    
    def decode(self, z_q: torch.Tensor):
        """
        ãƒ‡ã‚³ãƒ¼ãƒ€: é‡å­åŒ–ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã‹ã‚‰ç”»åƒã‚’å†æ§‹æˆ
        
        Parameters
        ----------
        z_q : torch.Tensor
            é‡å­åŒ–ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•° (B, emb_dim, h, w)
        
        Returns
        -------
        x_hat : torch.Tensor
            å†æ§‹æˆç”»åƒ (B, 3, 64, 64)
        """
        x_hat = self.conv_dec(z_q)
        return x_hat
    
    def forward(self, x: torch.Tensor):
        """
        é †ä¼æ’­
        
        Parameters
        ----------
        x : torch.Tensor
            å…¥åŠ›ç”»åƒ (B, 3, 64, 64)
        
        Returns
        -------
        x_hat : torch.Tensor
            å†æ§‹æˆç”»åƒ
        vq_loss : torch.Tensor
            ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–æå¤±
        encoding_indices : torch.Tensor
            é¸æŠã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        """
        z_e = self.encode(x)
        z_q, vq_loss, encoding_indices = self.quantize(z_e)
        x_hat = self.decode(z_q)
        
        return x_hat, vq_loss, encoding_indices
    
    def loss(self, x: torch.Tensor):
        """
        æå¤±é–¢æ•°ã®è¨ˆç®—
        
        Parameters
        ----------
        x : torch.Tensor
            å…¥åŠ›ç”»åƒ (batch_size, 3, 64, 64)
        
        Returns
        -------
        total_loss : torch.Tensor
            ç·æå¤±
        reconstruction_loss : torch.Tensor
            å†æ§‹æˆæå¤±
        vq_loss : torch.Tensor
            ãƒ™ã‚¯ãƒˆãƒ«é‡å­åŒ–æå¤±
        """
        x_hat, vq_loss, _ = self.forward(x)
        
        # å†æ§‹æˆæå¤±ï¼ˆMSEï¼‰
        reconstruction_loss = F.mse_loss(x_hat, x)
        
        # ç·æå¤±
        total_loss = reconstruction_loss + vq_loss
        
        return total_loss, reconstruction_loss, vq_loss
