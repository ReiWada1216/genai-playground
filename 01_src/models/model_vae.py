"""
VAEãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

def torch_log(x: torch.Tensor) -> torch.Tensor:
    """ torch.log(0)ã«ã‚ˆã‚‹nanã‚’é˜²ãç›®çš„ """
    return torch.log(torch.clamp(x, min=1e-10))

class VAE(nn.Module):
    """VAEãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…"""
    def __init__(self, z_dim:int) -> None:
        """
        Parameters
        z_dim : int
          æ½œåœ¨ç©ºé–“ã®æ¬¡å…ƒæ•°
        """
        super().__init__()

        #Encoder:  xã‚’å…¥åŠ›ã¨ã—ã¦ã€ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿mu, sigmaã‚’å‡ºåŠ›
        #å…¥åŠ›ç”»åƒï¼š[B, 3, 64, 64]
        self.conv_enc = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),  # -> [B, 64, 32, 32]
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1), # -> [B, 128, 16, 16]
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1), # -> [B, 256, 8, 8]
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, 4, 2, 1), # -> [B, 512, 4, 4]
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
        )
        """
        #å³å¯†ã«ã¯å®šç¾©ã—ãªãã¦ã„ã„ã‘ã©ã€ãã®å¾Œã®mu, sigmaã®linearã§å®šç¾©ã™ã‚‹ãŸã‚ã«å¿…è¦
        """
        self.enc_flat = 512*4*4
        self.enc_mu = nn.Linear(self.enc_flat, z_dim)
        self.enc_var = nn.Linear(self.enc_flat, z_dim)

        #Decoder: zã‚’å…¥åŠ›ã¨ã—ã¦ã€ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿mu, sigmaã‚’å‡ºåŠ›
        self.fc_dec = nn.Sequential(
            nn.Linear(z_dim, self.enc_flat),
            nn.BatchNorm1d(self.enc_flat), # 1æ¬¡å…ƒã®BatchNormã‚’è¿½åŠ 
            nn.ReLU(inplace=True)
        )
        self.conv_dec = nn.Sequential(
            nn.ConvTranspose2d(512, 256, 4, 2, 1), # -> [B, 256, 8, 8]
            nn.BatchNorm2d(256),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # -> [B, 128, 16, 16]
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64,  4, 2, 1),  # -> [B, 64, 32, 32]
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64,  3,   4, 2, 1),  # -> [B, 3, 64, 64]
            nn.Sigmoid()  # å‡ºåŠ›ã‚’ [0,1] ã«åã‚ã‚‹
        )

    def _encoder(self, x: torch.Tensor):
      """
      ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€xã‹ã‚‰å¹³å‡ã¨åˆ†æ•£ã‚’å‡ºåŠ›ã™ã‚‹ã¨ã“ã‚!!
      -------------
      Parameters
      x: torch.Tensor(b, c, h, w)
      """
      x = self.conv_enc(x)
      x = torch.flatten(x, start_dim=1) #ãƒãƒƒãƒã ã‘æ®‹ã™ã®ã§ã€start_dim=1
      mean = self.enc_mu(x) #å¹³å‡
      log_var = self.enc_var(x) #å¯¾æ•°åˆ†æ•£
      return mean, log_var

    def _sample_z(self, mean: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:
      """
      VAEã§ã¯ã€Œæ½œåœ¨å¤‰æ•° ğ‘§ã€ã‚’ç¢ºç‡çš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹.
      ã—ã‹ã—ã€ç¢ºç‡çš„ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ã¯å¾®åˆ†ãŒä¸å¯èƒ½ã«ãªã‚‹.
      ãã®ãŸã‚ã€å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãƒˆãƒªãƒƒã‚¯ãŒå¿…è¦ï¼ï¼
      -------------------
      Parameters
      mean: torch.Tensor(b, z_dim)
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒå‡ºåŠ›ã™ã‚‹ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¹³å‡
      log_var: torch.Tensor(b, z_dim)
        ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãŒå‡ºåŠ›ã™ã‚‹ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®å¯¾æ•°åˆ†æ•£
      """
      std = torch.exp(0.5 * log_var) #æ¨™æº–åå·®
      eps = torch.randn_like(std) #stdã¨åŒã˜å½¢ã®ä¹±æ•°ã‚’ç”Ÿæˆï¼(å¹³å‡0, åˆ†æ•£1)
      z = mean + std * eps
      return z

    def _decoder(self, z: torch.Tensor) -> torch.Tensor:
        """
        VAEã®ãƒ‡ã‚³ãƒ¼ãƒ€éƒ¨åˆ†:zã‚’å—ã‘å–ã‚Šã€xã‚’å†æ§‹æˆã™ã‚‹
        ----------
        Parameters
        z: torch.Tensor(b, z_dim)
          æ½œåœ¨å¤‰æ•°
        ----------
        returns
        x : torch.Tensor(b, c, h, w)
        """
        x = self.fc_dec(z)
        # x.viewã¯reshapeã¨åŒã˜åƒã
        x = x.view(-1, 512, 4, 4)  #-1ã¯ã€PyTorchã«ã€Œã“ã®æ¬¡å…ƒã¯è‡ªåˆ†ã§è¨ˆç®—ã—ã¦ã€ã¨ä¼ãˆã‚‹ç‰¹åˆ¥ãªå€¤ã€‚
        x = self.conv_dec(x)
        return x

    def forward(self, x: torch.Tensor):
        """
        é †ä¼æ’­
        ---------
        Parameters
        x : torch.Tensor ( b, c, h, w )
              å…¥åŠ›ç”»åƒï¼
        ---------
        Returns
        x_hat : torch.Tensor ( b, c, h, w )
              å†æ§‹æˆç”»åƒï¼
        z : torch.Tensor ( b, z_dim )
              æ½œåœ¨å¤‰æ•°ï¼
        """
        mean, log_var = self._encoder(x)
        z = self._sample_z(mean, log_var)
        x_hat = self._decoder(z)
        return x_hat, mean, log_var

    def loss(self, x: torch.Tensor):
        """
        ç›®çš„é–¢æ•°ã®è¨ˆç®—
        ----------
        parameters
        x: torch.Tensor(b, c, h, w)
          å…¥åŠ›ç”»åƒ
        ----------
        returns
        reconstruction: torch.Tensor(b, c, h, w)
          å†æ§‹æˆç”»åƒ
        KL: Torch.Tensor(, )
          æ­£å‰‡åŒ–, ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ï¼ˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰ã¨äº‹å‰åˆ†å¸ƒï¼ˆæ¨™æº–ã‚¬ã‚¦ã‚¹åˆ†å¸ƒï¼‰ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹
        """
        mean, log_var = self._encoder(x)
        z = self._sample_z(mean, log_var)
        x_hat = self._decoder(z)

        #-----KLã®ãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹, mean, std: (B, z_dim)
        #torch.meanã¯batch_sizeã«é–¢ã™ã‚‹ã‚‚ã®
        KL = -0.5 * torch.sum(1 + log_var - mean**2 - torch.exp(log_var)) / x.size(0)

        #----BCEæå¤±
        reconstruction = F.binary_cross_entropy(x_hat, x, reduction='sum') / x.size(0) #ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®èª¤å·®ã‚’å¹³å‡ã—ã¦ã„ã‚‹ 

        #----ç›®çš„é–¢æ•°
        loss = reconstruction + KL

        return loss, reconstruction, KL